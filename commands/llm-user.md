---
description: LLM user testing - setup, test, fix, and track
---

# LLM User Testing

**Command:** `/llm-user <subcommand>`

**Purpose:** Automated UI testing using LLM-simulated users to validate promises.

---

## Quick Reference

| Command | Purpose |
|---------|---------|
| `init` | Generate test infrastructure from L1 docs |
| `test` | Run LLM user tests against your UI |
| `fix` | Systematically fix gaps found by tests |
| `status` | View test results, gaps, and progress |
| `refresh` | Regenerate after doc changes |

---

## Workflow

```
/llm-user init     â†’ Generate personas, scenarios, evaluators
       â†“
/llm-user test     â†’ Run tests, find gaps
       â†“
/llm-user status   â†’ Review gaps by priority
       â†“
/llm-user fix      â†’ Fix gaps (auto-verifies after each)
       â†“
/llm-user test     â†’ Confirm all gaps resolved
```

---

## Commands

### `/llm-user init`

Generate test infrastructure from your L1 workflow docs.

```bash
/llm-user init           # Generate from docs
/llm-user init --force   # Regenerate even if exists
```

**Prerequisites:**
- `docs/intent/product-intent.md`
- `docs/ux/user-journeys.md`
- `docs/architecture/agent-design.md`

**Creates:**
```
tests/llm-user/
â”œâ”€â”€ test-spec.yaml       # Unified test configuration
â”œâ”€â”€ personas/            # Simulated user profiles
â””â”€â”€ scenarios/           # Test scenarios

.claude/agents/
â”œâ”€â”€ {{project}}-llm-user.md      # Domain-specific user agent
â””â”€â”€ {{project}}-evaluator.md     # Domain-specific evaluator
```

---

### `/llm-user test`

Run LLM user tests against your UI.

```bash
# Run all tests against localhost:3000
/llm-user test

# Test specific URL
/llm-user test https://staging.myapp.com

# Test specific scenario
/llm-user test --scenario=checkout-flow

# Test specific persona
/llm-user test --persona=maria-beginner

# Combine options
/llm-user test https://staging.myapp.com --scenario=checkout --persona=expert
```

**Options:**
| Option | Description |
|--------|-------------|
| `[url]` | Base URL (default: localhost:3000) |
| `--scenario=<id>` | Run specific scenario only |
| `--persona=<id>` | Run with specific persona only |
| `--critical` | Run only critical path scenarios |

**Output:**
```
LLM USER TEST RESULTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

URL: https://staging.myapp.com
Scenarios: 4 | Passed: 3 | Failed: 1

SCORE: 7.2/10

GAPS FOUND:
ğŸ”´ CRITICAL (1)
   GAP-001: No progress tracking visible

ğŸŸ  HIGH (2)
   GAP-002: Feedback not level-adaptive
   GAP-003: Loading time >3s

Run /llm-user status for details
Run /llm-user fix to resolve gaps
```

---

### `/llm-user fix`

Fix gaps found by LLM user testing. Auto-verifies after each fix.

```bash
# Fix all gaps (critical first)
/llm-user fix

# Fix only critical gaps
/llm-user fix --critical

# Fix only high priority
/llm-user fix --high

# Fix specific gap
/llm-user fix GAP-001

# Fix up to N gaps
/llm-user fix --limit=3
```

**Options:**
| Option | Description |
|--------|-------------|
| `[gap-id]` | Fix specific gap |
| `--critical` | Only critical gaps |
| `--high` | Critical and high gaps |
| `--limit=N` | Stop after N gaps |

**Workflow per gap:**
1. Create fix specification
2. Implement via workflow agents (backend/frontend/test)
3. Run code review
4. **Auto-verify** by re-running failed scenarios
5. Update gap status (OPEN â†’ CLOSED)
6. Move to next gap

**Output:**
```
FIXING GAPS
â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ GAP-001 (CRITICAL): No progress tracking
   Creating fix spec...
   Implementing 3 tasks...
   âœ“ Backend: progress tracking API
   âœ“ Frontend: progress dashboard
   âœ“ Tests: progress tracking tests

   Verifying...
   Re-running: multi-scene-session
   âœ“ Verification PASSED

   Status: OPEN â†’ CLOSED
   Score: 7.2 â†’ 8.5

Continue to GAP-002? [Y/n]
```

---

### `/llm-user status`

View test results, gaps, and fix progress.

```bash
# Show current status
/llm-user status

# Show specific test run
/llm-user status --run=2026-01-28T10:30:00

# Filter by priority
/llm-user status --critical
/llm-user status --high

# Filter by status
/llm-user status --open
/llm-user status --closed

# Export to file
/llm-user status --export
/llm-user status --export=json
```

**Options:**
| Option | Description |
|--------|-------------|
| `--run=<timestamp>` | View specific test run |
| `--critical` | Show only critical gaps |
| `--high` | Show critical and high |
| `--open` | Show only unresolved gaps |
| `--closed` | Show only resolved gaps |
| `--export` | Export to markdown file |
| `--export=json` | Export as JSON |

**Output:**
```
LLM USER TESTING STATUS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

LATEST RUN: 2026-01-28T10:30:00
SCORE: 8.5/10 (was 7.2)

PROMISE VALIDATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ“ P1: Scene descriptions work
âœ“ P2: Helpful corrections
âœ“ P3: Progress visible (was âœ—)
~ P4: Level-adaptive (in progress)

GAPS (2 open, 1 closed)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸŸ¢ CLOSED
   âœ“ GAP-001: Progress tracking

ğŸŸ  OPEN - HIGH
   â€¢ GAP-002: Feedback not adaptive
   â€¢ GAP-003: Loading time >3s

NEXT: /llm-user fix GAP-002
```

---

### `/llm-user refresh`

Regenerate test artifacts after workflow docs change.

```bash
# Detect changes and regenerate
/llm-user refresh

# Force full regeneration
/llm-user refresh --force
```

**When to use:**
- After updating `product-intent.md` (new promises)
- After updating `user-journeys.md` (new personas)
- After architecture changes

---

## Complete Example

```bash
# 1. After L1 planning, set up testing
/llm-user init
# âœ“ 3 personas, 5 scenarios generated

# 2. After L2 implementation, test the UI
/llm-user test https://staging.myapp.com
# Score: 7.2/10, 3 gaps found

# 3. Review what's wrong
/llm-user status
# Shows gaps by priority with recommendations

# 4. Fix the issues
/llm-user fix --critical
# Fixes GAP-001, auto-verifies, score improves

/llm-user fix --high
# Fixes GAP-002, GAP-003

# 5. Final verification
/llm-user test https://staging.myapp.com
# Score: 9.5/10, all promises validated

# 6. After doc updates, refresh
/llm-user refresh
# Regenerates test specs with new promises
```

---

## File Structure

```
tests/llm-user/
â”œâ”€â”€ test-spec.yaml           # Test configuration
â”œâ”€â”€ personas/
â”‚   â”œâ”€â”€ maria-beginner.yaml
â”‚   â”œâ”€â”€ jake-teen.yaml
â”‚   â””â”€â”€ sofia-expert.yaml
â”œâ”€â”€ scenarios/
â”‚   â”œâ”€â”€ first-scene.yaml
â”‚   â”œâ”€â”€ error-recovery.yaml
â”‚   â””â”€â”€ multi-scene.yaml
â””â”€â”€ fixes/                   # Fix specifications
    â””â”€â”€ GAP-001-progress.yaml

results/llm-user/
â””â”€â”€ 2026-01-28T10-30-00/
    â”œâ”€â”€ recordings/          # Session recordings
    â”œâ”€â”€ screenshots/         # Visual evidence
    â”œâ”€â”€ gap-analysis.md      # Human-readable
    â””â”€â”€ gap-analysis.json    # Machine-readable
```

---

## Best Practices

**DO:**
- Run `init` after L1 planning completes
- Test early and often during L2
- Fix critical gaps before release
- Re-test after fixes to confirm

**DON'T:**
- Skip `init` (tests need setup first)
- Ignore critical gaps (they block release)
- Manually edit generated files (use `refresh`)
- Test without L1 docs (garbage in, garbage out)

---

## Troubleshooting

**"Test spec not found"**
â†’ Run `/llm-user init` first

**"Required docs not found"**
â†’ Complete L1 planning first (`/intent`, `/ux`, `/architect`)

**"All tests pass but users complain"**
â†’ Check personas have realistic frustration thresholds
â†’ Run `/llm-user refresh` to regenerate

**"Fix verification keeps failing"**
â†’ Review fix specification
â†’ May need different implementation approach

---

## Related

- `/review` - Code review (different from UX testing)
- `/verify` - Run verification checks
- `/debug` - Debug specific issues
