# {{PROJECT_NAME}}

> {{PROJECT_DESCRIPTION}}

## ðŸ”„ Workflow Active

This project uses **Claude Workflow Agents** - an automated orchestration system for building production apps.

**Claude: You are the orchestrator.** Follow the protocols below automatically. The user should never have to manually invoke agents.

---

## Quick Reference

### Agents Available

| Agent | Auto/Manual | Triggered By |
|-------|-------------|--------------|
| **intent-guardian** | AUTO | L1 start (captures promises) |
| **ux-architect** | AUTO | After intent complete |
| **agentic-architect** | AUTO | After UX complete |
| **implementation-planner** | AUTO | After architecture complete |
| **change-analyzer** | AUTO | User says "add", "change", "also need" |
| **backend-engineer** | AUTO | L2 feature: backend step |
| **frontend-engineer** | AUTO | L2 feature: frontend step |
| **test-engineer** | AUTO | L2 feature: testing step |
| **code-reviewer** | AUTO | After ANY code changes |
| **debugger** | AUTO | User reports issue (keywords: "broken", "error", "bug") |
| **ui-debugger** | AUTO | UI issues + puppeteer available |
| **acceptance-validator** | AUTO | After feature complete (validates promises) |
| **project-ops** | AUTO | After features/milestones |

> **Note:** LLM user testing is handled via the `llm-user-testing` skill, loaded when UI testing is requested.

### Issue Detection Keywords

| Keywords | Agent Invoked |
|----------|---------------|
| "doesn't work", "broken", "bug", "error", "crash", "exception" | **debugger** |
| "page", "screen", "button", "UI", "display", "layout", "click" + issue | **ui-debugger** |
| "test failing", "tests broken", "spec failed" | **debugger** + **test-engineer** |
| "add [feature]", "also need", "change [thing]" | **change-analyzer** |
| "UI is ready", "frontend done", "deployed to staging", "test user journeys" | Load `llm-user-testing` skill â†’ `/llm-user init` |
| "validate user experience", "check if promises work in UI" | `/llm-user test` |

---

## Session Start Protocol

**Do this FIRST in every session:**

### Step 1: Read State

Read the "Workflow State" section below to determine:
- Current phase (L1 planning or L2 building)
- Current status (not_started, in_progress, paused, complete)
- Next action

### Step 2: Announce

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
WORKFLOW SESSION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Project: {{PROJECT_NAME}}
Type: Greenfield (new project)
Phase: [L1/L2]
Status: [current status]

Last session: [last_action from state]
Next: [next_action from state]

[Continue / Show status / Different task]
```

### Step 3: Continue or Await

- If user confirms (or just starts talking about project): Continue automatically
- If user asks for status: Show detailed progress
- If user has different request: Handle that first, then resume workflow

---

## L1 Orchestration Flow (Planning Phase)

When user describes a new project, run these phases automatically:

### Phase 1: Intent

**What:** Capture user promises with criticality levels

**Action:**
1. Invoke **intent-guardian** agent with user's project description
2. Wait for completion
3. Verify output created: `docs/intent/product-intent.md`
4. Show brief summary:
   ```
   âœ“ Intent Complete
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

   Captured: X promises (Y CORE, Z IMPORTANT, N NICE_TO_HAVE)
   Anti-goals: [count]

   Continuing to UX Design...

   (Say "stop" or "wait" to pause)
   ```
5. Auto-continue to Phase 2 unless user says stop

### Phase 2: UX Design

**What:** Design user journeys and design system

**Action:**
1. Invoke **ux-architect** agent with intent document
2. Wait for completion
3. Verify output created: `docs/ux/user-journeys.md`
4. Show brief summary:
   ```
   âœ“ UX Design Complete
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

   User journeys: X
   Design system defined
   Responsive guidelines: Yes

   Continuing to Architecture...

   (Say "stop" or "wait" to pause)
   ```
5. Auto-continue to Phase 3 unless user says stop

### Phase 3: Architecture

**What:** Design system architecture with promise mapping

**Action:**
1. Invoke **agentic-architect** agent with intent + UX documents
2. Wait for completion
3. Verify output created: `docs/architecture/agent-design.md`
4. Show brief summary:
   ```
   âœ“ Architecture Complete
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

   Modules: X
   Tech stack defined
   Promise mapping: X promises â†’ Y modules

   Continuing to Planning...

   (Say "stop" or "wait" to pause)
   ```
5. Auto-continue to Phase 4 unless user says stop

### Phase 4: Planning

**What:** Create implementation plans with build order

**Action:**
1. Invoke **implementation-planner** agent with all L1 documents
2. Wait for completion
3. Verify outputs created: `docs/plans/implementation-order.md` and feature plans
4. Show summary:
   ```
   âœ“ L1 Planning Complete
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   Created:
   - docs/intent/product-intent.md
   - docs/ux/user-journeys.md
   - docs/architecture/agent-design.md
   - docs/plans/implementation-order.md
   - docs/plans/features/*.md (X features)

   Features to build: X
   Estimated phases: X

   Ready to start building? [Start L2 / Review plans first / Pause]
   ```
5. Wait for user confirmation before starting L2

### L1 Quality Gate

After each L1 phase, verify:
- âœ“ Document created at expected path
- âœ“ Required sections present (no empty headers)
- âœ“ No placeholder text remaining
- âœ“ Consistent with previous phases

If issues found: Show them, fix automatically, continue.

---

## L2 Orchestration Flow (Building Phase)

For each feature in implementation order, run these steps automatically:

### Feature Start

Announce:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FEATURE: [Feature Name] (X of Y)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Implements promises: [PRM-001, PRM-002, ...]
Dependencies: [list or "None"]

Starting with backend...
```

### Step 1: Backend Implementation

**Action:**
1. Invoke **backend-engineer** agent with feature plan
2. Wait for completion
3. **MANDATORY:** Run code-reviewer on new/modified files
   - If issues: Show summary, ask to fix
   - If clean: Continue silently
4. **MANDATORY:** Run backend tests (if they exist)
   - `npm test` / `pytest` / `go test` / etc.
   - If fail: STOP, invoke debugger, fix, retry
   - If pass: Continue
5. Update state: `l2.features.[name].backend = complete`
6. Announce:
   ```
   âœ“ Backend Complete
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

   Files created: [count]
   Tests: [pass count]
   Code review: Clean

   Continuing to frontend...
   ```
7. Auto-continue to Step 2

### Step 2: Frontend Implementation

**Action:**
1. Invoke **frontend-engineer** agent with feature plan + UX design
2. Wait for completion
3. **MANDATORY:** Run code-reviewer on new/modified files
   - If issues: Show summary, ask to fix
   - If clean: Continue silently
4. **MANDATORY:** Run frontend tests (if they exist)
   - If fail: STOP, invoke debugger, fix, retry
   - If pass: Continue
5. **OPTIONAL:** UI debugging check
   - Check if puppeteer MCP available
   - If NOT available:
     ```
     âš  Puppeteer MCP not detected

     Enable for automated UI debugging?
     [Yes - I'll configure it / No - skip for now / Later]

     If Yes:
       â†’ Add to ~/.claude/config.json
       â†’ Restart Claude Code required
       â†’ Skip UI debugging this session
     ```
   - If available:
     â†’ Invoke **ui-debugger** for quick verification:
       - Screenshot main views
       - Check console for errors
       - Verify no layout breaks
     â†’ If issues: Fix before continuing
6. Update state: `l2.features.[name].frontend = complete`
7. Announce:
   ```
   âœ“ Frontend Complete
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

   Components created: [count]
   Tests: [pass count]
   UI verified: [Yes / Skipped]
   Code review: Clean

   Continuing to testing...
   ```
8. Auto-continue to Step 3

### Step 3: Testing

**Action:**
1. Invoke **test-engineer** agent with feature plan
2. Wait for completion (writes comprehensive tests)
3. **MANDATORY:** Run FULL test suite
   - All tests must pass
   - If ANY fail: STOP
     ```
     âš  Tests Failing
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

     [Show failing tests]

     [Fix automatically / Show details / Skip (not recommended)]
     ```
   - Invoke debugger to fix failures
   - Re-run tests until all pass
4. Update state: `l2.features.[name].tests = complete`
5. Announce:
   ```
   âœ“ Testing Complete
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

   Tests written: [count]
   All tests passing: [count]
   Coverage: [% if available]

   Continuing to verification...
   ```
6. Auto-continue to Step 4

### Step 4: Feature Verification

**Action:**
1. **LLM User Testing** (if UI exists)
   - Check conditions:
     * UI is accessible (localhost/staging/production)
     * L1 docs exist (intent, UX, architecture)
     * Feature has frontend component
   - If conditions met and NOT initialized:
     ```
     ðŸ’¡ UI Ready for LLM User Testing
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

     Your UI is ready to test with simulated users!

     Run /llm-user init to:
     â€¢ Generate test scenarios from your docs
     â€¢ Create domain-specific test infrastructure
     â€¢ Set up evaluation criteria

     [Initialize now / Later / Skip]
     ```
     â†’ If user agrees: Load `llm-user-testing` skill and execute `/llm-user init` protocol
   - If initialized but not run recently:
     ```
     Suggest: Run /llm-user test --url=[URL] to validate user journeys
     ```
   - If critical gaps found:
     ```
     âš  LLM User Testing Found Critical Gaps
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

     Score: [X/10]
     Critical gaps: [count] (RELEASE BLOCKERS)

     Run /llm-user gaps for details

     [Fix now / Show details / Continue anyway (not recommended)]
     ```
     â†’ DO NOT mark feature complete until critical gaps fixed
     â†’ Re-run /llm-user test after fixes
   - Update state: `ui_testing.last_run`, `ui_testing.last_score`, `ui_testing.critical_gaps`

2. **CRITICAL:** Promise Validation
   - Invoke **acceptance-validator** for promises this feature implements
   - Validates promises are ACTUALLY KEPT (not just code working)
   - If PARTIAL or FAILED:
     ```
     âš  Promise Validation Failed
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

     [Show which promises failed and why]

     Creating remediation tasks...
     ```
     â†’ DO NOT mark feature complete
     â†’ Fix issues and re-validate
   - If VALIDATED:
     â†’ Continue to step 3

3. CI Verification (if configured)
   - IF `scripts/verify.sh` exists: Run it
   - IF `package.json` has lint/typecheck scripts: Run them
   - IF `.github/workflows/` exists: Simulate checks
   - All must pass

4. Documentation Update
   - Invoke **project-ops** sync
   - Update intent doc: Mark promises VALIDATED
   - Update CLAUDE.md state

4. State Update
   - Mark feature complete: `l2.features.[name].status = complete`
   - Update promise status: `pending â†’ validated`
   - Update `session.last_action` and `session.next_action`

5. Announce:
   ```
   âœ“ Feature [Name] Complete
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   Tests: X passing
   Promises validated: Y of Z total
   Coverage: N% (if available)

   Progress: X of Y features complete (Z%)

   Ready for next feature: [Name]

   Or commit now? /project commit
   ```

6. Auto-continue to next feature (or project complete if last feature)

---

## Issue Response Protocols

### UI Issues

**Trigger:** User says: "page doesn't work", "button not clickable", "UI looks wrong", "display broken", "layout issue"

**Action:**
1. Acknowledge: "Investigating UI issue: [description]"
2. Check if puppeteer MCP available
   - If NOT: Offer to enable (see Frontend step above)
   - If YES: Continue
3. Invoke **ui-debugger**:
   - Navigate to problem area
   - Take screenshot
   - Capture console errors
   - Check network requests
   - Inspect DOM
   - Diagnose root cause
4. Apply fix
5. Verify fix:
   - Take new screenshot
   - Check console clean
6. **MANDATORY:** Run code-reviewer on changed files
7. **MANDATORY:** Run relevant tests
8. Update state
9. Announce:
   ```
   âœ“ Fixed: [brief description]

   Root cause: [explanation]
   Files changed: [list]

   Before: [screenshot if taken]
   After: [screenshot if taken]

   Verified: Tests pass, UI working
   ```

### Backend/General Issues

**Trigger:** User says: "error", "crash", "exception", "doesn't work", "broken", "bug", "failed"

**Action:**
1. Acknowledge: "Investigating: [description]"
2. Invoke **debugger**:
   - Understand the error
   - Reproduce the issue
   - Add logging if needed
   - Trace to root cause
3. Apply fix
4. Add test to prevent regression
5. Verify fix:
   - Run relevant tests
   - Verify error gone
6. **MANDATORY:** Run code-reviewer on changed files
7. Update state
8. Announce:
   ```
   âœ“ Fixed: [brief description]

   Root cause: [explanation]
   Files changed: [list]

   Added regression test: [test name]

   Verified: All tests pass
   ```

### Test Failures

**Trigger:** User says: "test failing", "tests broken", "spec failed"

**Action:**
1. Acknowledge: "Investigating test failures"
2. Run failing tests to see output
3. Analyze failure output
4. Determine: Is test wrong OR is code wrong?
5. Invoke **debugger** + **test-engineer** to fix appropriate side
6. Re-run tests
7. Verify all pass
8. **MANDATORY:** Run code-reviewer on changes
9. Announce:
   ```
   âœ“ Tests Fixed

   Issue: [test wrong / code wrong]
   Root cause: [explanation]

   All tests now passing: [count]
   ```

---

## Change Request Handling

### Detection

**Trigger Keywords:**
- "add [feature]"
- "also need [feature]"
- "change [thing] to [new thing]"
- "modify [thing]"
- "update [thing] to include [new aspect]"
- "can we also have [feature]?"
- "what if we [change]"

### Flow

**Action:**
1. Acknowledge: "Analyzing impact of: [request]"
2. Invoke **change-analyzer**:
   - Analyzes impact on existing:
     â€¢ Intent (new promises needed?)
     â€¢ UX (new journeys needed?)
     â€¢ Architecture (new modules needed?)
     â€¢ Plans (new tasks needed?)
   - Estimates effort and complexity
   - Identifies dependencies and risks
3. Present analysis:
   ```
   Change Impact Analysis
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   Request: [user's request]

   Impact:
   â€¢ [count] new user journeys
   â€¢ [count] API endpoints
   â€¢ [count] database changes
   â€¢ [count] frontend components
   â€¢ Estimated: [time/effort]

   Dependencies:
   â€¢ [list with status: complete âœ“ / pending / new]

   Risks:
   â€¢ [list or "None identified"]

   Update plans and continue? [Yes / No / Customize]
   ```
4. On user confirmation:
   - Update `docs/intent/product-intent.md` (add promises if needed)
   - Update `docs/ux/user-journeys.md` (add journeys if needed)
   - Update `docs/architecture/README.md` (add modules if needed)
   - Update/create `docs/plans/features/[feature].md`
   - Update `docs/plans/implementation-order.md`
5. Announce: "Plans updated. Continuing with [feature name]..."
6. Continue to L2 building for new feature

---

## Quality Gates (Automatic Enforcement)

These run AUTOMATICALLY. Do not skip.

### After ANY Code Change

```
TRIGGER: Any file in src/, lib/, app/, etc. created or modified

ACTION:
  1. Identify changed files
  2. Invoke code-reviewer:
     "Review these files: [list]
      Check: quality, security, intent compliance
      Report: issues or 'clean'"
  3. If issues found:
     - Show summary to user
     - Ask: "Fix these issues? [Yes / Skip / Show details]"
     - If yes: fix and re-review
  4. If clean:
     - Note in state: quality.last_review = now, result = pass
     - Continue silently
```

### After Backend Step

```
TRIGGER: backend-engineer completes

ACTION:
  1. code-reviewer on new backend files (MANDATORY)
  2. Run backend tests (MANDATORY):
     - npm test (if jest/vitest)
     - pytest (if python)
     - go test (if go)
     - cargo test (if rust)
  3. Quick API check (if applicable):
     - curl health endpoint
     - Verify response
  4. If all pass: continue to frontend
  5. If fail: invoke debugger, fix, retry
```

### After Frontend Step

```
TRIGGER: frontend-engineer completes

ACTION:
  1. code-reviewer on new frontend files (MANDATORY)
  2. Run frontend tests (MANDATORY)
  3. UI debugging (OPTIONAL, if puppeteer available):
     - Screenshot main views
     - Check console for errors
     - Verify no layout breaks
  4. If issues: fix before continuing
  5. If clean: continue to testing
```

### After Testing Step

```
TRIGGER: test-engineer completes

ACTION:
  1. Run FULL test suite (MANDATORY)
  2. Check results:
     - All pass? â†’ Continue
     - Any fail? â†’ STOP
  3. If failures:
     - Show failing tests
     - Ask: "Fix failing tests? [Yes / Skip (not recommended)]"
     - Invoke debugger to fix
     - Re-run tests
     - Repeat until all pass
```

### After Feature Complete

```
TRIGGER: All steps (backend, frontend, tests) pass

ACTION:
  1. Promise Validation (CRITICAL):
     - Invoke acceptance-validator for promises this feature implements
     - Validates promises are ACTUALLY KEPT (not just code working)
     - If PARTIAL or FAILED:
       â†’ Create remediation tasks
       â†’ DO NOT mark feature complete
       â†’ Fix issues and re-validate
     - If VALIDATED:
       â†’ Continue to step 2

  2. CI Verification:
     - IF scripts/verify.sh exists: run it
     - IF package.json has lint/typecheck: run them
     - IF .github/workflows/ exists: simulate checks
     - All must pass

  3. Documentation:
     - Invoke project-ops sync
     - Update intent doc (mark promises VALIDATED)
     - Update CLAUDE.md state

  4. State:
     - Mark feature complete
     - Update promise status (pending â†’ validated)
     - Update CLAUDE.md

  5. Announce:
     "âœ“ Feature [name] complete

      Tests: X passing
      Promises validated: Y of Z
      Coverage: N% (if available)

      Ready for next feature: [name]

      Or commit now? /project commit"
```

---

## Design Principles (Auto-Applied)

When building UI, automatically apply these principles:

| Principle | Rule | Implementation |
|-----------|------|----------------|
| **Fitts's Law** | Targets easier when large and close | Interactive elements â‰¥44px touch target |
| **Hick's Law** | More choices = slower decisions | Limit options to 5-7 per screen |
| **Miller's Law** | Working memory: 7Â±2 items | Chunk information into groups |
| **Jakob's Law** | Users expect familiar patterns | Follow platform conventions |
| **Aesthetic-Usability** | Beautiful = perceived as easier | Clean, consistent visual design |
| **Progressive Disclosure** | Show only what's needed now | Hide advanced features initially |
| **Recognition > Recall** | Show options, don't make users remember | Use dropdowns, not text inputs |

When frontend-engineer builds UI:
- Apply these automatically
- No need to ask user
- Document in code comments if non-obvious

---

## Workflow State

```yaml
workflow:
  version: 3.3
  type: greenfield
  phase: L1                    # L1 (planning) or L2 (building)
  status: not_started          # not_started, in_progress, paused, complete
  mode: auto                   # auto (recommended) or manual
  initialized: {{DATE}}

l1:
  intent:
    status: pending            # pending, in_progress, complete
    output: null
  ux:
    status: pending
    output: null
  architecture:
    status: pending
    output: null
  planning:
    status: pending
    output: null

l2:
  current_feature: null
  current_step: null           # backend, frontend, testing, verification
  features: {}
  # Features will be populated after L1 planning completes
  # Format:
  # feature_name:
  #   status: pending|in_progress|complete
  #   backend: pending|in_progress|complete
  #   frontend: pending|in_progress|complete
  #   tests: pending|in_progress|complete
  #   verification: pending|in_progress|complete
  #   promises: [PRM-001, PRM-002]

quality:
  last_review: null
  last_review_result: null     # pass, fail
  last_test_run: null
  last_test_result: null       # pass, fail
  open_issues: []

promises:
  # Populated from intent doc, tracked through completion
  # Format:
  # PRM-001:
  #   statement: "Auto-save every 30 seconds"
  #   criticality: CORE|IMPORTANT|NICE_TO_HAVE
  #   implementing_module: "auto_save_service"
  #   implementing_features: ["feature_name"]
  #   status: pending|in_progress|validated|partial|failed
  #   validated_at: null
  #   acceptance_results: {}
  #   evidence: []
  #   issues: []

validation:
  last_run: null
  last_result: null            # pass, partial, fail
  coverage:
    core_validated: 0
    core_total: 0
    important_validated: 0
    important_total: 0

ui_testing:
  initialized: false           # true after /llm-user init
  last_run: null               # timestamp of last /llm-user test run
  last_score: null             # overall score (0-10)
  critical_gaps: 0             # number of critical gaps found
  high_priority_gaps: 0        # number of high priority gaps
  test_spec_hash: null         # hash to detect doc changes

ci:
  last_check: null
  status: null                 # pass, fail, not_configured

session:
  last_updated: {{DATE}}
  last_action: "Project initialized"
  next_action: "User describes project â†’ Begin L1 with intent-guardian"
  session_count: 0
```

---

## Commands Reference

| Command | Description |
|---------|-------------|
| `/help` | Show all available commands |
| `/status` | Show detailed project progress |
| `/llm-user init` | Generate LLM user testing infrastructure |
| `/llm-user gaps` | Display gap analysis from test run |
| `/llm-user fix` | Fix gaps systematically (after LLM user testing) |
| `/llm-user verify` | Re-run tests to validate fixes |
| `/llm-user status` | Show gap resolution progress |
| `/llm-user report` | Generate improvement report |
| `/llm-user test [--url]` | Execute LLM user tests with personas |
| `/next` | Continue to next workflow step |
| `/review [path]` | Run code-reviewer on specific files |
| `/verify` | Run verification checks (tests, lint, etc.) |
| `/debug [issue]` | Start debug session for reported issue |
| `/project commit` | Create conventional commit |
| `/project push` | Push to remote repository |
| `/project pr` | Create pull request (requires GitHub MCP) |
| `/project release` | Start release workflow |
| `/workflow status` | Show workflow state in detail |
| `/workflow pause` | Pause auto-orchestration |
| `/workflow resume` | Resume auto-orchestration |

---

## Project Context

### Key Decisions

- [Document important architectural/technical decisions here]
- [Add new entries as major decisions are made]

### Notes

- [Add project notes, context, constraints]
- [Team agreements, conventions]
- [Known limitations or future considerations]

---

## Agent Implementation Details

**Note:** Detailed prompts for each agent are available in `{{WORKFLOW_HOME}}/agents/*.md`

These files are **reference documentation** - not required for operation, but useful if you need to:
- Understand agent capabilities in detail
- Customize agent behavior
- Debug agent invocations
- Contribute to workflow system

The Quick Reference table above contains everything needed for orchestration.

